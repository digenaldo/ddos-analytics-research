# -*- coding: utf-8 -*-
"""DDoS-Attack-Analytics-Digenaldo-with-Spark.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zLxCEbsdrj3aVYTEt8PlmYeYOYLGbtAM
"""

import findspark

findspark.init()

from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark import SparkConf

# Creating a SparkSession
spark = SparkSession.builder \
    .appName("DDoS Attack Data Analysis") \
    .getOrCreate()
# Cria a configuração do Spark
# conf = SparkConf() \
#     .setAppName("DDoS Attack Data Analysis") \
#     .setMaster("local[*]") \
#     .set("spark.executor.memory", "70g") \
#     .set("spark.driver.memory", "50g") \
#     .set("spark.memory.offHeap.enabled", "true") \
#     .set("spark.memory.offHeap.size", "16g")

# # Inicializa o SparkContext com a configuração
# sc = SparkContext(conf=conf)

# # Define o nível de log para ERROR
# sc.setLogLevel("ERROR")

# # Cria a sessão do Spark
# spark = SparkSession(sc)

# Load DataFrames
train_data = spark.read.csv("train_mosaic.csv", header=True, inferSchema=True)
test_data = spark.read.csv("test_mosaic.csv", header=True, inferSchema=True)

from pyspark.sql.functions import when, col

feature_cols_replaces = [
    "Flow_IAT_Mean",
    "Flow_IAT_Std",
    "Flow_IAT_Max",
    "Flow_IAT_Min",
    "Fwd_IAT_Total",
    "Fwd_IAT_Mean",
    "Fwd_IAT_Std",
    "Fwd_IAT_Max",
    "Fwd_IAT_Min",
    "Bwd_IAT_Total",
    "Bwd_IAT_Mean",
    "Bwd_IAT_Std",
    "Bwd_IAT_Max",
    "Bwd_IAT_Min",
    "Fwd_PSH_Flags",
    "Bwd_PSH_Flags",
    "Fwd_URG_Flags",
    "Bwd_URG_Flags",
    "Fwd_Header_Length",
    "Bwd_Header_Length",
    "Fwd_Packets_Sec",
    "Bwd_Packets_Sec",
    "Min_Packet_Length",
    "Max_Packet_Length",
    "Packet_Length_Mean",
    "Packet_Length_Std",
    "Packet_Length_Variance",
    "FIN_Flag_Count",
    "SYN_Flag_Count",
    "RST_Flag_Count",
    "PSH_Flag_Count",
    "ACK_Flag_Count",
    "URG_Flag_Count",
    "CWE_Flag_Count",
    "ECE_Flag_Count",
    "Down_Up_Ratio",
    "Average_Packet_Size",
    "Avg_Fwd_Segment_Size",
    "Avg_Bwd_Segment_Size",
    "Fwd_Avg_Bytes_Bulk",
    "Fwd_Avg_Packets_Bulk",
    "Fwd_Avg_Bulk_Rate",
    "Bwd_Avg_Bytes_Bulk",
    "Bwd_Avg_Packets_Bulk",
    "Bwd_Avg_Bulk_Rate",
    "Subflow_Fwd_Packets",
    "Subflow_Fwd_Bytes",
    "Subflow_Bwd_Packets",
    "Subflow_Bwd_Bytes",
    "Init_Win_bytes_forward",
    "Init_Win_bytes_backward",
    "act_data_pkt_fwd",
    "min_seg_size_forward",
    "Active_Mean",
    "Active_Std",
    "Active_Max",
    "Active_Min",
    "Idle_Mean",
    "Idle_Std",
    "Idle_Max",
    "Idle_Min"
]

def replace_values_less_than_zero(df, replacement_value=0.0):
    """Replaces all values less than 0 with a replacement value in specified columns of a DataFrame."""
    for col_name in feature_cols_replaces:
        df = df.withColumn(
            col_name,
            when(col(col_name) < 0, replacement_value).otherwise(col(col_name))
        )
    return df

train_data = replace_values_less_than_zero(train_data)
test_data = replace_values_less_than_zero(test_data)

#Display the DataFrames after the substitution
train_data.select(feature_cols_replaces).show()
test_data.select(feature_cols_replaces).show()

from pyspark.sql.functions import col, isnan, lit

def filter_invalid_values(df, feature_cols):
    """Filters rows with invalid values (negative, NaN, infinity, empty string) in specified columns."""
    invalid_conditions = [
            col(col_name).isin(float("inf"), float("-inf"), "") | isnan(col(col_name)) | (col(col_name) < 0)  # Include negative check
            for col_name in feature_cols
    ]
    combined_condition = invalid_conditions[0]
    for condition in invalid_conditions[1:]:
        combined_condition = combined_condition | condition
    return df.filter(combined_condition)

# Filter invalid values in both train and test data (assuming test_data_selected exists)
train_invalid_values = filter_invalid_values(train_data, feature_cols_replaces)
test_invalid_values = filter_invalid_values(test_data, feature_cols_replaces)

# Show filtered data (optional)
train_invalid_values.select(feature_cols_replaces).show()
test_invalid_values.select(feature_cols_replaces).show()

from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer

# Convert the "Label" column to a numeric type using StringIndexer
label_indexer = StringIndexer(inputCol="Label", outputCol="label")
train_data_indexed = label_indexer.fit(train_data).transform(train_data)

# Convert the "Label" column to a numeric type using StringIndexer for test_data
label_indexer_test = StringIndexer(inputCol="Label", outputCol="label")
test_data_indexed = label_indexer_test.fit(test_data).transform(test_data)

# Identify feature columns
feature_cols = [
    "Destination_Port",
    "Total_Fwd_Packets",
    "Total_Backward_Packets",
    "Total_Length_of_Fwd_Packets",
    "Total_Length_of_Bwd_Packets",
    "Fwd_Packet_Length_Max",
    "Fwd_Packet_Length_Min",
    "Fwd_Packet_Length_Mean",
    "Fwd_Packet_Length_Std",
    "Bwd_Packet_Length_Max",
    "Bwd_Packet_Length_Min",
    "Bwd_Packet_Length_Mean",
    "Bwd_Packet_Length_Std",
    "Flow_Bytes_Sec",
    "Flow_Packets_Sec",
    "Flow_IAT_Mean",
    "Flow_IAT_Std",
    "Flow_IAT_Max",
    "Flow_IAT_Min",
    "Fwd_IAT_Total",
    "Fwd_IAT_Mean",
    "Fwd_IAT_Std",
    "Fwd_IAT_Max",
    "Fwd_IAT_Min",
    "Bwd_IAT_Total",
    "Bwd_IAT_Mean",
    "Bwd_IAT_Std",
    "Bwd_IAT_Max",
    "Bwd_IAT_Min",
    "Fwd_PSH_Flags",
    "Bwd_PSH_Flags",
    "Fwd_URG_Flags",
    "Bwd_URG_Flags",
    "Fwd_Header_Length",
    "Bwd_Header_Length",
    "Fwd_Packets_Sec",
    "Bwd_Packets_Sec",
    "Min_Packet_Length",
    "Max_Packet_Length",
    "Packet_Length_Mean",
    "Packet_Length_Std",
    "Packet_Length_Variance",
    "FIN_Flag_Count",
    "SYN_Flag_Count",
    "RST_Flag_Count",
    "PSH_Flag_Count",
    "ACK_Flag_Count",
    "URG_Flag_Count",
    "CWE_Flag_Count",
    "ECE_Flag_Count",
    "Down_Up_Ratio",
    "Average_Packet_Size",
    "Avg_Fwd_Segment_Size",
    "Avg_Bwd_Segment_Size",
    "Fwd_Avg_Bytes_Bulk",
    "Fwd_Avg_Packets_Bulk",
    "Fwd_Avg_Bulk_Rate",
    "Bwd_Avg_Bytes_Bulk",
    "Bwd_Avg_Packets_Bulk",
    "Bwd_Avg_Bulk_Rate",
    "Subflow_Fwd_Packets",
    "Subflow_Fwd_Bytes",
    "Subflow_Bwd_Packets",
    "Subflow_Bwd_Bytes",
    "Init_Win_bytes_forward",
    "Init_Win_bytes_backward",
    "act_data_pkt_fwd",
    "min_seg_size_forward",
    "Active_Mean",
    "Active_Std",
    "Active_Max",
    "Active_Min",
    "Idle_Mean",
    "Idle_Std",
    "Idle_Max",
    "Idle_Min"
]

# Create a VectorAssembler to combine features
vector_assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# Combine features for training data
train_features = vector_assembler.transform(train_data_indexed)

# Combine features for test data
test_features = vector_assembler.transform(test_data_indexed)

# Select only the feature columns and the label column
selected_cols = feature_cols + ["label", "features"]  # Add "features" to the selected columns list

# Select only relevant columns for training data
train_features_selected = train_features.select(selected_cols)

# Select only relevant columns for test data
test_features_selected = test_features.select(selected_cols)

# Check the schema of the train_features_selected DataFrame
train_features_selected.printSchema()

# Check the schema of the test_features_selected DataFrame
test_features_selected.printSchema()

from pyspark.ml.classification import NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from sklearn.metrics import classification_report
from pyspark.sql import SparkSession
import numpy as np

# Create a Naive Bayes classifier, specifying the label and features columns
nb = NaiveBayes(featuresCol="features", labelCol="label", smoothing=1e-9)

# Train the model with prepared training data
model = nb.fit(train_features_selected)

# Make predictions on the test data
predictions = model.transform(test_features_selected)

# Extract the actual labels and predictions from the Spark DataFrame
y_true = np.array(predictions.select("label").collect())
y_pred = np.array(predictions.select("prediction").collect())

# Convert the labels back to the original format (optional)
y_true = np.squeeze(y_true)
y_pred = np.squeeze(y_pred)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Generate and print the classification report
print("\nClassification Report for Naive Bayes:")
print(classification_report(y_true, y_pred))

print("Accuracy:", accuracy)

from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from sklearn.metrics import classification_report
from pyspark.sql import SparkSession
import numpy as np

# Create a Decision Tree classifier
dt = DecisionTreeClassifier(featuresCol="features", labelCol="label", maxDepth=5, maxBins=32)

# Train the model
model = dt.fit(train_features_selected)

# Make predictions
predictions = model.transform(test_features_selected)

# Extract the actual labels and predictions
y_true = np.array(predictions.select("label").collect())
y_pred = np.array(predictions.select("prediction").collect())

# Convert the labels back to the original format (optional)
y_true = np.squeeze(y_true)
y_pred = np.squeeze(y_pred)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Generate and print the classification report
print("\nClassification Report for Decision Tree:")
print(classification_report(y_true, y_pred))

print("Accuracy:", accuracy)

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from sklearn.metrics import classification_report
from pyspark.sql import SparkSession
import numpy as np

# Create a logistic regression model
lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10)

# Train the model
model = lr.fit(train_features_selected)

# Make predictions
predictions = model.transform(test_features_selected)

# Extract the actual labels and predictions
y_true = np.array(predictions.select("label").collect())
y_pred = np.array(predictions.select("prediction").collect())

# Convert the labels back to the original format (optional)
y_true = np.squeeze(y_true)
y_pred = np.squeeze(y_pred)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Generate and print the classification report
print("\nClassification Report for Logistic Regression:")
print(classification_report(y_true, y_pred))

print("Accuracy:", accuracy)

from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from sklearn.metrics import classification_report
from pyspark.sql import SparkSession
import numpy as np

# Create a Random Forest model
rf = RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10, maxDepth=5)

# Train the model
model = rf.fit(train_features_selected)

# Make predictions
predictions = model.transform(test_features_selected)

# Extract the actual labels and predictions
y_true = np.array(predictions.select("label").collect())
y_pred = np.array(predictions.select("prediction").collect())

# Convert the labels back to the original format (optional)
y_true = np.squeeze(y_true)
y_pred = np.squeeze(y_pred)

# Evaluate the model
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(predictions)

# Generate and print the classification report
print("\nClassification Report for Random Forest:")
print(classification_report(y_true, y_pred))

print("Accuracy:", accuracy)

from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.sql import SparkSession
import numpy as np
import time
import matplotlib.pyplot as plt

# Lista de classificadores
classifiers = {
    "Naive Bayes": NaiveBayes(featuresCol="features", labelCol="label", smoothing=1e-9),
    "Decision Tree": DecisionTreeClassifier(featuresCol="features", labelCol="label", maxDepth=5, maxBins=32),
    "Logistic Regression": LogisticRegression(featuresCol="features", labelCol="label", maxIter=10),
    "Random Forest": RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10, maxDepth=5)
}

# Dicionário para armazenar os tempos de execução
execution_times = {}
# Inicializar listas para armazenar os tempos de execução e as acurácias
results = []

# Função para adicionar resultados
def add_results(algorithm, environment, times, accuracies, iteration_times, iteration_accuracies, run):
    results.append({
        "rodada": run,
        "algoritmo": algorithm,
        "ambiente": environment,
        "tempo": times[-1],
        "acuracia": accuracies[-1],
        "iteracoes_tempo": iteration_times,
        "iteracoes_acuracia": iteration_accuracies
    })
    
# Loop sobre os classificadores
# for clf_name, clf_model in classifiers.items():
#     start_time = time.time()
#     # Treinar o modelo
#     model = clf_model.fit(train_features_selected)
#     # Avaliar o modelo
#     predictions = model.transform(test_features_selected)
#     evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
#     accuracy = evaluator.evaluate(predictions)
#     end_time = time.time()
#     execution_time = end_time - start_time
#     # Armazenar o tempo de execução
#     execution_times[clf_name] = execution_time
#     # Imprimir os resultados
#     print(f"Classifier: {clf_name}")
#     print(f"Accuracy: {accuracy}")
#     print(f"Execution Time: {execution_time} seconds")
#     print("\n")

from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.feature import StringIndexer
from pyspark.sql import SparkSession
import numpy as np
import time
import matplotlib.pyplot as plt

# Lista de classificadores
classifiers = {
    "Naive Bayes": NaiveBayes(featuresCol="features", labelCol="label", smoothing=1e-9),
    "Decision Tree": DecisionTreeClassifier(featuresCol="features", labelCol="label", maxDepth=5, maxBins=32),
    "Logistic Regression": LogisticRegression(featuresCol="features", labelCol="label", maxIter=10),
    "Random Forest": RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10, maxDepth=5)
}

# Dicionário para armazenar as acurácias
accuracies = {}

# Loop sobre os classificadores
# for clf_name, clf_model in classifiers.items():
#     # Treinar o modelo
#     model = clf_model.fit(train_features_selected)
#     # Avaliar o modelo
#     predictions = model.transform(test_features_selected)
#     evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
#     accuracy = evaluator.evaluate(predictions)
#     # Armazenar a acurácia
#     accuracies[clf_name] = accuracy
#     # Imprimir os resultados
#     print(f"Classifier: {clf_name}")
#     print(f"Accuracy: {accuracy}")
#     print("\n")

from pyspark.ml.classification import NaiveBayes, DecisionTreeClassifier, LogisticRegression, RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql import SparkSession
import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt


# Lista de classificadores
classifiers = {
    "nb": NaiveBayes(featuresCol="features", labelCol="label", smoothing=1e-9),
    "dt": DecisionTreeClassifier(featuresCol="features", labelCol="label", maxDepth=5, maxBins=32),
    "lr": LogisticRegression(featuresCol="features", labelCol="label", maxIter=10),
    "rf": RandomForestClassifier(featuresCol="features", labelCol="label", numTrees=10, maxDepth=5)
}

# Dicionários para armazenar os tempos de execução e as acurácias
execution_times = {clf_name: [] for clf_name in classifiers}
accuracies = {clf_name: [] for clf_name in classifiers}

# Executar cada classificador dez vezes
# for iteration in range(10):
#     print(f"Iteração {iteration}")
#     for clf_name, clf_model in classifiers.items():
#         start_time = time.time()
#         # Treinar o modelo
#         model = clf_model.fit(train_features_selected)
#         # Avaliar o modelo
#         predictions = model.transform(test_features_selected)
#         evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
#         accuracy = evaluator.evaluate(predictions)
#         end_time = time.time()
#         execution_time = end_time - start_time
#         # Armazenar o tempo de execução e a acurácia
#         execution_times[clf_name].append(execution_time)
#         accuracies[clf_name].append(accuracy)
#         # Imprimir resultados
#         print(f"Acurácia ({clf_name}): {accuracy}, Tempo de Execução: {execution_time} segundos")
#         print()

# Executar cada classificador dez vezes
for iteration in range(5):
    print(f"Iteração {iteration+1}")
    for clf_name, clf_model in classifiers.items():
        start_time = time.time()
        # Treinar o modelo
        model = clf_model.fit(train_features_selected)
        # Avaliar o modelo
        predictions = model.transform(test_features_selected)
        evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
        accuracy = evaluator.evaluate(predictions)
        end_time = time.time()
        execution_time = end_time - start_time
        # Armazenar o tempo de execução e a acurácia
        execution_times[clf_name].append(execution_time)
        accuracies[clf_name].append(accuracy)
        # Imprimir resultados
        print(f"Acurácia ({clf_name}): {accuracy}, Tempo de Execução: {execution_time} segundos")

        # Loop para 30 execuções por classificador
        run_times = []
        run_accuracies = []
        for run in range(5):
            start_time = time.time()
            model = clf_model.fit(train_features_selected)
            predictions = model.transform(test_features_selected)
            accuracy = evaluator.evaluate(predictions)
            end_time = time.time()
            run_times.append(end_time - start_time)
            run_accuracies.append(accuracy)
            print(f"    Execução {run+1} para o classificador {clf_name}")
            print(f"    Tempo de execução: {run_times[-1]} segundos")
            print(f"    Acurácia: {run_accuracies[-1]}")

        # Adicionar resultados
        add_results(clf_name, "bd", execution_times[clf_name], accuracies[clf_name], run_times, run_accuracies, iteration+1)
        print()  # Adiciona uma linha em branco após as execuções do classificador
    print()  # Adiciona uma linha em branco após cada iteração

# Converte a lista de resultados em um DataFrame
df_results = pd.DataFrame(results)
print(df_results)

# Exporta o DataFrame para um arquivo CSV
df_results.to_csv("resultados_bd.csv", index=False)

spark.stop()